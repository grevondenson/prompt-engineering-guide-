The framework can allow LLMs to interact with external tools to retrieve additional information  that leads to more reliable and factual responses.
Results show that ReAct can outperform several state-of-the-art baselines on language and decision making tasks.
ReAct also leads to improved human Interpretability and tustworthiness of LLMs.
Overall, the authors found the best approach uses ReAct combined with chain-of-thought that allows use of both internal knowledge and external information obtained during reasoning.

    HOW IT WORKS 
    
    It is inspired by the synergies between "acting" and "reasoning" which allow humans to learn new tasks and make decisions or reasoning.
    ReAct is a general paradigm that combines reasoning and acting with LLMs.
    ReAct prompts LLMs to generate verbal reasoning traces and actions for a task.
    This allows the system to perform dynamic reasoning to create, maintain and adjust plans for acting while also enabling interaction to external enviroments to incoporate additional information into the reasoning.


To demostrate ReAct prompting works, let's follow an example;

The first step is to select cases from a training set and compose ReAct format trajectories.
These are used as few-shot examplars in the prompts.
The trajectories consist of multiple thought-action-observation steps.
The free-form thoughts are used to achieve different task such as decomposing questions, extracting information, performing commonsense/arithmetic reasoning, guide search formulation and synthesizing final answer.

Here is an example of what the ReAct prompt exemplars look like 

    Question What is the elevation range for the area that the eastern sector of the
    Colorado orogeny extends into?
    Thought 1 I need to search Colorado orogeny, find the area that the eastern sector
    of the Colorado orogeny extends into, then find the elevation range of the
    area.
    Action 1 Search[Colorado orogeny]
    Observation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in
    Colorado and surrounding areas.
    Thought 2 It does not mention the eastern sector. So I need to look up eastern
    sector.
    Action 2 Lookup[eastern sector]
    Observation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called
    the Central Plains orogeny.
    Thought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I
    need to search High Plains and find its elevation range.
    Action 3 Search[High Plains]
    Observation 3 High Plains refers to one of two distinct land regions
    Thought 4 I need to instead search High Plains (United States).
    Action 4 Search[High Plains (United States)]
    Observation 4 The High Plains are a subregion of the Great Plains. From east to west, the
    High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130
    m).[3]
    Thought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer
    is 1,800 to 7,000 ft.
    Action 5 Finish[1,800 to 7,000 ft]
    ...


In summary;
    CoT suffers from fact hallucinations
    ReAct's structural constrain reduces its flexibility in formulating reasoning steps.
    ReAct depends a lot on the information it's retriving; non-informative search results derails the model reasoning and leads to difficulty in recovering and reformulating thoughts.

Prompting methods that combine and support switching between ReAct and CoT+self consistency generally outperform all other prompting methods.


    LANGCHAIN REACT USAGE 

Below is a high-level example of how ReAct prompting approach works in practice.
First, let's install and import the necessary libraries:

    %%capture
    # update or install the necessary libraries
    !pip install --upgrade openai
    !pip install --upgrade langchain
    !pip install --upgrade python-dotenv
    !pip install google-search-results
    
    # import libraries
    import openai
    import os
    from langchain.llms import OpenAI
    from langchain.agents import load_tools
    from langchain.agents import initialize_agent
    from dotenv import load_dotenv
    load_dotenv()
    
    # load API keys; you will need to obtain these if you haven't yet
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
    os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY")
 

Now we can configure the LLM, the tools we will use, and the agent that allows us to leverage the ReAct framework together with the LLM and tools.
Note that we are using a search api for searching external information and LLM as a math tool.

    llm = OpenAI(model_name="text-davinci-003" ,temperature=0)
    tools = load_tools(["google-serper", "llm-math"], llm=llm)
    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)

Once that is configured we can run the agent with the disired query/prompt.
Notice that we are not expected to provide few-shot exemplars 

    agent.run("Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?")

The chain execution looks as follows 

    > Entering new AgentExecutor chain...
    I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.
    Action: Search
    Action Input: "Olivia Wilde boyfriend"
    Observation: Olivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis â€” see their relationship timeline.
    Thought: I need to find out Harry Styles' age.
    Action: Search
    Action Input: "Harry Styles age"
    Observation: 29 years
    Thought: I need to calculate 29 raised to the 0.23 power.
    Action: Calculator
    Action Input: 29^0.23
    Observation: Answer: 2.169459462491557
    
    Thought: I now know the final answer.
    Final Answer: Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557.
    
    > Finished chain.

The output we get is as follows 

    "Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557."

I encourage the learner to explore different combinations of tools and tasks.