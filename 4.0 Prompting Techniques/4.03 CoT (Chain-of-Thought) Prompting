CoT enables complex reasoning capabilities through intermideate reasoning steps.
You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.

    prompt
        The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.
        The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.
        A: Adding all the odd numbers (17, 19) gives 36. The answer is True.
        The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.
        A: Adding all the odd numbers (11, 13) gives 24. The answer is True.
        The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.
        A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 

    Output
        Adding all the odd numbers (15, 5, 13, 7, 1) gives 41.
        the answer is False

We can see a perfect result when we provide the reasoning step.
In fact, we can solve this task by providing even fewer exampples i.e just one example seems enough.

    prompt
        The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.

        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. 
    
    Output 
        Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.

Keep in mind that the authors claim that this is an emergent ability that arises with sufficiently large language models.


        ZERO SHOT CoT PROMPTING 
    
    It essentially involves adding "let's think step by step" to the original prompt.
    Let's try an example problem;

        prompt
            I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?

        Output
            11 apples

    The answer is incorrect! 
    Now let's try with the special prompt.

        Prompt 
            I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?
            Let's think step by step.

        Output
            First, you started with 10 apples.
            You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.
            Then you bought 5 more apples, so now you had 11 apples.
            Finally, you ate 1 apple, so you would remain with 10 apples.

    It's impressive that this simple prompt is effective at this task.
    This is particulary useful where you don't have too many examples to use in the prompt.


    AUTOMATIC CHAIN-OF THOUGHT (AUTO CoT)

    This samples questions with diversity and generates reasoning chains to construct the demonstrations.
    It has two main stages;

        Stage 1
        Question clustering 
            Partion questions of a given dataset into a few clusters

        Stage 2
        demonstration sampling 
            select a representative question from each cluster and generate its reasoning chain using zero-shot-CoT with simple heuristics.

    The simple heuristics could be length of questions (e.g 60 tokens) and number of steps in rationale (e.g 5 reasoning steps).
    This encourages the model to use simple and accurate demonstrations.
    

